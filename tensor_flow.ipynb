{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import constant from TensorFlow\n",
    "from tensorflow import Variable, constant,multiply, matmul, keras, add,float32, ones,random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_numpy=pd.read_csv('uci_credit_card.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foundamental tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor constants, example Bias \n",
    "&emsp;[Difference btw Tensor constants and variables](https://thecleverprogrammer.com/2021/03/19/difference-between-tensors-and-arrays/)<br>\n",
    "Tensor Variables, example Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_constant = constant(credit_numpy)\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = credit_constant.numpy()\n",
    "\n",
    "a1=tf.Variable([range(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor calculation\n",
    "add() , multiply() , matmul() , and reduce_sum()<BR>gradient() , reshape() , and random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " C23: [[1 2 3]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Define tensors A1 and A23 as constants\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "B23 =  tf.ones_like(A23 )\n",
    "# Perform element-wise multiplication\n",
    "C0 = add(A23,B23)\n",
    "C23 = multiply(A23,B23)\n",
    "#C_matmul=matmul(A23,B23)\n",
    "print('\\n C23: {}'.format(C23.numpy()))\n",
    "# Summing over tensor dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(-1.0)\n",
    "# Define y within instance of GradientTape \n",
    "with tf.GradientTape() as tape: \n",
    "    tape.watch(x) \n",
    "    y = tf.multiply(x, x)\n",
    "# Evaluate the gradient of y at x = -1 \n",
    "g = tape.gradient(y, x)\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate grayscale image \n",
    "gray = tf.random.uniform([2, 2], maxval=255, dtype='int32')\n",
    "gray = tf.reshape(gray, [2*2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models\n",
    "1. define tensor_variables: intercept, slope \n",
    "2. define linear_regression=intercept+slop*features\n",
    "3. define loss function: keras.losses.mse(targets, predictions)\n",
    "4. optimizer under several cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13400/1519914271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mslope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprice_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'size_log' is not defined"
     ]
    }
   ],
   "source": [
    "intercept=Variable(5.0, float32)\n",
    "slope=Variable(0.001, float32)\n",
    "# 1. Model\n",
    "def linear_regression(intercept, slope, features = size_log):\n",
    "\treturn add(intercept,multiply(slope,features))\n",
    "# 2. Loss function\n",
    "def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "\treturn keras.losses.mse(targets, predictions)\n",
    "# 3. Initialize an Adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "for j in range(100):\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "# Print the trained parameters \n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Multiple updates per epoch\n",
    "2. Requires division of dataset\n",
    "3. No limit on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level approach: linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "inputs = constant([[1.0, 35.0]])\n",
    "weights = Variable([[-0.05], [-0.01]])\n",
    "bias = Variable(0.5)\n",
    "product = tf.matmul(inputs, weights)\n",
    "dense=keras.activations.sigmoid(product+bias)\n",
    "#dense is not a layer(function) but a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional_High-level approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. Specify Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first dense layer \n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "# Define second dense layer \n",
    "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
    "# Define output (predictions) layer \n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output layer for Multiclass classification problems\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Activation functions<br>\n",
    "    1.1 Linear: Matrix multiplication<br>\n",
    "    1.2 Nonlinear: Activation function<br>\n",
    "Why nonlinearities are important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizers<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD<br>\n",
    "tf.keras.optimizers.SGD<br> learning_rate<p>\n",
    "RMSprop<br>\n",
    "Applies di learning rates to each feature<br>\n",
    "tf.keras.optimizers.RMSprop()<br> \n",
    "learning_rate momentum<br> learning rate is related to Avoiding local minima\n",
    "decay<p>\n",
    "Adam<br>\n",
    "Adaptive moment (adam) optimizer \n",
    "<br>tf.keras.optimizers.Adam() \n",
    "<br>learning_rate beta1\n",
    "<br>Performs well with default parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSprop Optimizer example<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "for j in range(100):\n",
    "\t# fit model with minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# fit model with  minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing variables in TensorFlow\n",
    "1. Normal \n",
    "2. Uniform\n",
    "3. Glorot initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "# Define 500x500 random normal variable \n",
    "weights = tf.Variable(tf.random.normal([500, 500]))\n",
    "# Define 500x500 truncated random normal variable\n",
    "weights = tf.Variable(tf.random.truncated_normal([500, 500]))\n",
    "\n",
    "# Define a dense layer with the default initializer \n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "# Define a dense layer with the zeros initializer \n",
    "dense_initial = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functional API： \n",
    " train two models jointly to predict the same target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer+ Output layer\n",
    "input_shape=(784,) uses Input(shape=(1,)) which returns a tensor\n",
    "output_layer=Dense(1) is a function, which takes a tensor in and produce a tensor out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'borrower_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13400/3950541802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm1_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mborrower_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mm2_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mborrower_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mm1_layer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm1_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mm1_layer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm1_layer1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'borrower_features' is not defined"
     ]
    }
   ],
   "source": [
    "m1_inputs = np.array(borrower_features, np.float32)\n",
    "m2_inputs = np.array(borrower_features, np.float32)\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functional vs sequential<br>\n",
    "<img src=\"https://miro.medium.com/max/873/1*Xs31AY9jnmoAw3F0o6Jv1w.png\" width=\"300\"/><br>\n",
    "sequential model must define input dimension<br>\n",
    "A Sequential model is not appropriate when:<br>\n",
    "1. Your model has multiple inputs or multiple outputs<br>\n",
    "2. Any of your layers has multiple inputs or multiple outputs<br>\n",
    "3. You need to do layer sharing<br>\n",
    "4. You want non-linear topology (e.g. a residual connection, a multi-branch model)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('slmnist.csv',header=None)\n",
    "X=df.drop(columns=0)\n",
    "y=pd.get_dummies(df[0])\n",
    "sign_language_features, X_test, sign_language_labels, y_test = train_test_split(\n",
    "X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models<p>\n",
    "'categorical_crossentropy' loss function <br>\n",
    "Similar to log loss: Lower is be <br>\n",
    "Add metrics = ['accuracy'] to compile step for easy-tounderstand diagnostics <br>\n",
    "Output layer has separate node for each possible outcome,<br>\n",
    "and uses 'softmax' activation <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Specify Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Model\n",
    "model = keras.Sequential()\n",
    "# Define the input dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "# Define the hidden dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4,activation='softmax'))\n",
    "# Print the model architecture \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compile <p>\n",
    "what does compiling mean?<br>\n",
    "which sets up the network for optimization, for instance creating an internal function to do back-propagation efficiently.\n",
    "Two arguments:<br>\n",
    "a. Optimizer algorithm: versatile algorithm <br>\n",
    "b. loss function<br>\n",
    "    'mean_squared_error' common for regression<br>\n",
    "    'categorical_crossentropy' for classification<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit<p>\n",
    "Applying backpropagation and gradient descent with your data to update the weights<br>\n",
    "Scaling data before  can ease optimization<p>\n",
    " batch_size <br> \n",
    " epochs <br> \n",
    " validation_split<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 1ms/step - loss: 1.3793\n",
      "21/21 [==============================] - 0s 949us/step - loss: 1.3852\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(sign_language_features, sign_language_labels)\n",
    "large_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a numeric feature column \n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "# Define a categorical feature column \n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\",[\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "# Create feature column list \n",
    "feature_list = [size, rooms]\n",
    "# Define a matrix feature column \n",
    "features_list = [tf.feature_column.numeric_column('image', shape=(784,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deep neural network regression \n",
    "model0 = tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10, 6, 6, 3])\n",
    "model0.train(input_fn, steps=20)\n",
    "\n",
    "# Define a deep neural network classifier \n",
    "model1 = tf.estimator.DNNClassifier(feature_columns=feature_list,hidden_units=[32, 16, 8], n_classes=4)\n",
    "# Train the classifier\n",
    "model1.train(input_fn, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneously optimizing 1000s of parameters with complex relationships <br>\n",
    "Updates may not improve model meaningfully <br>\n",
    "Updates too small (if learning rate is low) or too large (if learning rate is high)<br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "330204c9d8b03d73c01b77e5fa072b00779363ad44c9b37d26bce407274881dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
